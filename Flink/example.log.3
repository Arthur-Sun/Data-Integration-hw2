INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@67e12de8
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@22c246a6
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@2f0f266c
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 (2028c78f2b3b51fb0c5716e81687b870) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 (16b056aecb06ad390a5cc2ca7516379f) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (8cf021b54e3b5b8e8e7fe23431c6e4c2) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 9a59def82f31209296ae5196f07dc0f7.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 (ea5faaa941a54f020d1f17ef6c82eb3c) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 (88331f4217251894e33e0c0cc691b012) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 (8dcd934d2d02ff3f74c7e013216b6922) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8) (2028c78f2b3b51fb0c5716e81687b870) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 0bc0cd710ff7cddaa410acec58f6a874.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 7bddc566a404b442879be2520bcc494b.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 (a49a175d5626e3c997a56db3ca4d8bc3) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@54634df9
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8) (16b056aecb06ad390a5cc2ca7516379f) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 0bb239b0c7e849fd8b9a93a46781b64b.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 40c308ccbcc69381f701c458e66a08cd.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 63ba79dd72b0a75ba9a578e8187cb9db.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 1fd2dc52eb2ded036fc4079110d4f335.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (8cf021b54e3b5b8e8e7fe23431c6e4c2) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot fab125d31cb4779e2b9cc025d23ae1e6.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot fab125d31cb4779e2b9cc025d23ae1e6.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8) (ea5faaa941a54f020d1f17ef6c82eb3c) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8) (88331f4217251894e33e0c0cc691b012) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8) (8dcd934d2d02ff3f74c7e013216b6922) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8) (a49a175d5626e3c997a56db3ca4d8bc3) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8) (8cf021b54e3b5b8e8e7fe23431c6e4c2) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (be08aa081eb382edeb9da34437288266), deploy into slot with allocation id fab125d31cb4779e2b9cc025d23ae1e6.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (be08aa081eb382edeb9da34437288266) switched from CREATED to DEPLOYING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (be08aa081eb382edeb9da34437288266) [DEPLOYING].
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@4fbf0d6c
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (be08aa081eb382edeb9da34437288266) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8) (be08aa081eb382edeb9da34437288266) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 1 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 4 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 5 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 6 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 2 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 0 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 7 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 3 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
WARN Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.NetworkClient - Connection to node -1 could not be established. Broker may not be available.
INFO TaskExecutorLocalStateStoresManager shutdown hook org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager - Shutting down TaskExecutorLocalStateStoresManager.
INFO TransientBlobCache shutdown hook org.apache.flink.runtime.blob.TransientBlobCache - Shutting down BLOB cache
INFO PermanentBlobCache shutdown hook org.apache.flink.runtime.blob.PermanentBlobCache - Shutting down BLOB cache
INFO FileChannelManagerImpl-io shutdown hook org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager removed spill file directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-io-1d5772e0-122c-4f71-8d06-ec6b9b38b01b
INFO FileCache shutdown hook org.apache.flink.runtime.filecache.FileCache - removed file cache directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-dist-cache-ad4822bc-277c-4ef3-962a-f782f650d24f
INFO FileChannelManagerImpl-netty-shuffle shutdown hook org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager removed spill file directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-netty-shuffle-c2cc864c-0a9b-4dd5-9c47-f684d12dc2f7
INFO BlobServer shutdown hook org.apache.flink.runtime.blob.BlobServer - Stopped BLOB server at 0.0.0.0:51617
INFO main KafkaConsumerApp - hello,world.what are you fucking?
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.
INFO main org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Starting Flink Mini Cluster
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Starting Metrics Registry
INFO main org.apache.flink.runtime.metrics.MetricRegistryImpl - No metrics reporter configured, no metrics will be exposed/reported.
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Starting RPC Service(s)
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Trying to start local actor system
INFO flink-akka.actor.default-dispatcher-3 akka.event.slf4j.Slf4jLogger - Slf4jLogger started
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka://flink
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Trying to start local actor system
INFO flink-metrics-2 akka.event.slf4j.Slf4jLogger - Slf4jLogger started
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils - Actor system started at akka://flink-metrics
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Starting high-availability services
INFO main org.apache.flink.runtime.blob.BlobServer - Created BLOB server storage directory C:\Users\孙宇鹏\AppData\Local\Temp\blobStore-227ef19c-62d0-43d0-93d8-b7dcfc9901c1
INFO main org.apache.flink.runtime.blob.BlobServer - Started BLOB server at 0.0.0.0:52092 - max concurrent requests: 50 - max backlog: 1000
INFO main org.apache.flink.runtime.blob.PermanentBlobCache - Created BLOB cache storage directory C:\Users\孙宇鹏\AppData\Local\Temp\blobStore-42555629-1938-41ac-99f4-8ffafdb3696e
INFO main org.apache.flink.runtime.blob.TransientBlobCache - Created BLOB cache storage directory C:\Users\孙宇鹏\AppData\Local\Temp\blobStore-94710179-f965-4003-b46a-80a9fe5fc657
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Starting 1 TaskManger(s)
INFO main org.apache.flink.runtime.taskexecutor.TaskManagerRunner - Starting TaskManager with ResourceID: dfef5c3e-4644-4234-9e64-919d5a74bd07
INFO main org.apache.flink.runtime.taskexecutor.TaskManagerServices - Temporary file directory 'C:\Users\孙宇鹏\AppData\Local\Temp': total 135 GB, usable 15 GB (11.11% usable)
INFO main org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-io-109db344-eb4d-48df-9342-bd8fa3f7336a for spill files.
INFO main org.apache.flink.runtime.io.disk.FileChannelManagerImpl - FileChannelManager uses directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-netty-shuffle-1fb6cf9a-6462-4e31-9052-3b8fd5b92a98 for spill files.
INFO main org.apache.flink.runtime.io.network.buffer.NetworkBufferPool - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).
INFO main org.apache.flink.runtime.io.network.NettyShuffleEnvironment - Starting the network environment and its components.
INFO main org.apache.flink.runtime.taskexecutor.KvStateService - Starting the kvState service and its components.
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka://flink/user/rpc/taskmanager_0 .
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Start job leader service.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.filecache.FileCache - User file cache uses directory C:\Users\孙宇鹏\AppData\Local\Temp\flink-dist-cache-2fff4948-75d3-4a94-9478-858e4c19483a
INFO main org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Starting rest endpoint.
WARN main org.apache.flink.runtime.webmonitor.WebMonitorUtils - Log file environment variable 'log.file' is not set.
WARN main org.apache.flink.runtime.webmonitor.WebMonitorUtils - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
INFO main org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Rest endpoint listening at localhost:52128
INFO main org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender http://localhost:52128
INFO main org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - Web frontend listening at http://localhost:52128.
INFO mini-cluster-io-thread-1 org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint - http://localhost:52128 was granted leadership with leaderSessionID=f5f0553c-6f66-4610-9d08-c669924c72f0
INFO mini-cluster-io-thread-1 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader http://localhost:52128 , session=f5f0553c-6f66-4610-9d08-c669924c72f0
INFO main org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
INFO main org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Starting the resource manager.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: StandaloneResourceManager
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - ResourceManager akka://flink/user/rpc/resourcemanager_1 was granted leadership with fencing token ad67a391da705c3da7c80d27f54c4b81
INFO main org.apache.flink.runtime.minicluster.MiniCluster - Flink Mini Cluster started successfully
INFO mini-cluster-io-thread-2 org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Start SessionDispatcherLeaderProcess.
INFO mini-cluster-io-thread-5 org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Recover all persisted job graphs.
INFO mini-cluster-io-thread-5 org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess - Successfully recovered 0 persisted job graphs.
INFO mini-cluster-io-thread-6 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/resourcemanager_1 , session=a7c80d27-f54c-4b81-ad67-a391da705c3d
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(ad67a391da705c3da7c80d27f54c4b81).
INFO mini-cluster-io-thread-5 org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_2 .
INFO mini-cluster-io-thread-5 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/dispatcher_2 , session=b90929d0-42af-411d-a905-294c2820e5d1
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Resolved ResourceManager address, beginning registration
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering TaskManager with ResourceID dfef5c3e-4644-4234-9e64-919d5a74bd07 (akka://flink/user/rpc/taskmanager_0) at ResourceManager
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.taskexecutor.TaskExecutor - Successful registration at resource manager akka://flink/user/rpc/resourcemanager_1 under registration id a7bc29f2379396e3d2fffb5258e43689.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Received JobGraph submission 3c7eb5d2b238e88660086ded8df2cab0 (consumer).
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.dispatcher.StandaloneDispatcher - Submitting job 3c7eb5d2b238e88660086ded8df2cab0 (consumer).
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
INFO jobmanager-future-thread-1 org.apache.flink.runtime.rpc.akka.AkkaRpcService - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_3 .
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Initializing job consumer (3c7eb5d2b238e88660086ded8df2cab0).
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Using restart back off time strategy NoRestartBackoffTimeStrategy for consumer (3c7eb5d2b238e88660086ded8df2cab0).
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Running initialization on master for job consumer (3c7eb5d2b238e88660086ded8df2cab0).
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Successfully ran initialization on master in 0 ms.
INFO jobmanager-future-thread-1 org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology - Built 8 pipelined regions in 1 ms
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@5a45e991
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Checkpoint storage is set to 'jobmanager'
INFO jobmanager-future-thread-1 org.apache.flink.runtime.checkpoint.CheckpointCoordinator - No checkpoint found during restore.
INFO jobmanager-future-thread-1 org.apache.flink.runtime.jobmaster.JobMaster - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@90bc950 for consumer (3c7eb5d2b238e88660086ded8df2cab0).
INFO jobmanager-future-thread-1 org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_3 , session=a220bcb5-5104-45ea-8ffa-462e5f202b47
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.jobmaster.JobMaster - Starting execution of job consumer (3c7eb5d2b238e88660086ded8df2cab0) under job master id 8ffa462e5f202b47a220bcb5510445ea.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.jobmaster.JobMaster - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Job consumer (3c7eb5d2b238e88660086ded8df2cab0) switched from state CREATED to RUNNING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8) (b4c8639dab83f8b49aeea6eb2d326ac5) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8) (d47d6c0a835426f2fb0e299cdd849b91) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8) (dd50a8b7ca589e10920eb3397cb2ac9a) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8) (c3d9aeada70e62e32b083bfeb30f5b22) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8) (a315543da7d12f808d3f554fb8925d81) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8) (441e95451059bc35c9657196f7fdd738) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8) (95daf7b0d22323bd8cbb466609de7a45) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8) (3b6a34431045d6ae02f4acdf325be9b8) switched from CREATED to SCHEDULED.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.jobmaster.JobMaster - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_1(ad67a391da705c3da7c80d27f54c4b81)
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.jobmaster.JobMaster - Resolved ResourceManager address, beginning registration
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registering job manager 8ffa462e5f202b47a220bcb5510445ea@akka://flink/user/rpc/jobmanager_3 for job 3c7eb5d2b238e88660086ded8df2cab0.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Registered job manager 8ffa462e5f202b47a220bcb5510445ea@akka://flink/user/rpc/jobmanager_3 for job 3c7eb5d2b238e88660086ded8df2cab0.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.jobmaster.JobMaster - JobManager successfully registered at ResourceManager, leader id: ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager - Received resource requirements from job 3c7eb5d2b238e88660086ded8df2cab0: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=8}]
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 7245d44b3a0d964ff0ad67ea1b4b9732 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 7245d44b3a0d964ff0ad67ea1b4b9732.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Add job 3c7eb5d2b238e88660086ded8df2cab0 for job leader monitoring.
INFO mini-cluster-io-thread-17 org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Try to register at job manager akka://flink/user/rpc/jobmanager_3 with leader id a220bcb5-5104-45ea-8ffa-462e5f202b47.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request e6c0f3ef4532332ad5fcd476d994899b for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for e6c0f3ef4532332ad5fcd476d994899b.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 003fdcaadfd178da3b6bbe02f84a3e08 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-4 org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Resolved JobManager address, beginning registration
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 003fdcaadfd178da3b6bbe02f84a3e08.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 7693dd9bcd746b8f80444bd4dce4db49 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 7693dd9bcd746b8f80444bd4dce4db49.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request fd65528b142e9c9fc19b68b5576bcc24 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for fd65528b142e9c9fc19b68b5576bcc24.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request f201de2af4761175dbad50d2e4890168 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for f201de2af4761175dbad50d2e4890168.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 0d5d849d9b1e25c938da99e7ddc5c140 for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 0d5d849d9b1e25c938da99e7ddc5c140.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Receive slot request 89ac6a3d19e1dd1198ab199aecf497cc for job 3c7eb5d2b238e88660086ded8df2cab0 from resource manager with leader id ad67a391da705c3da7c80d27f54c4b81.
INFO flink-akka.actor.default-dispatcher-6 org.apache.flink.runtime.taskexecutor.TaskExecutor - Allocated slot for 89ac6a3d19e1dd1198ab199aecf497cc.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService - Successful registration at job manager akka://flink/user/rpc/jobmanager_3 for job 3c7eb5d2b238e88660086ded8df2cab0.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Establish JobManager connection for job 3c7eb5d2b238e88660086ded8df2cab0.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Offer reserved slots to the leader of job 3c7eb5d2b238e88660086ded8df2cab0.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8) (b4c8639dab83f8b49aeea6eb2d326ac5) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8) (attempt #0) with attempt id b4c8639dab83f8b49aeea6eb2d326ac5 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id 7245d44b3a0d964ff0ad67ea1b4b9732
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8) (d47d6c0a835426f2fb0e299cdd849b91) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8) (attempt #0) with attempt id d47d6c0a835426f2fb0e299cdd849b91 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id e6c0f3ef4532332ad5fcd476d994899b
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 7245d44b3a0d964ff0ad67ea1b4b9732.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8) (dd50a8b7ca589e10920eb3397cb2ac9a) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8) (attempt #0) with attempt id dd50a8b7ca589e10920eb3397cb2ac9a to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id 003fdcaadfd178da3b6bbe02f84a3e08
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8) (c3d9aeada70e62e32b083bfeb30f5b22) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8) (attempt #0) with attempt id c3d9aeada70e62e32b083bfeb30f5b22 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id f201de2af4761175dbad50d2e4890168
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8) (a315543da7d12f808d3f554fb8925d81) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8) (attempt #0) with attempt id a315543da7d12f808d3f554fb8925d81 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id 89ac6a3d19e1dd1198ab199aecf497cc
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8) (441e95451059bc35c9657196f7fdd738) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8) (attempt #0) with attempt id 441e95451059bc35c9657196f7fdd738 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id 7693dd9bcd746b8f80444bd4dce4db49
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8) (95daf7b0d22323bd8cbb466609de7a45) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8) (attempt #0) with attempt id 95daf7b0d22323bd8cbb466609de7a45 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id fd65528b142e9c9fc19b68b5576bcc24
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8) (3b6a34431045d6ae02f4acdf325be9b8) switched from SCHEDULED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-3 org.apache.flink.runtime.executiongraph.ExecutionGraph - Deploying Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8) (attempt #0) with attempt id 3b6a34431045d6ae02f4acdf325be9b8 to dfef5c3e-4644-4234-9e64-919d5a74bd07 @ kubernetes.docker.internal (dataPort=-1) with allocation id 0d5d849d9b1e25c938da99e7ddc5c140
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 (b4c8639dab83f8b49aeea6eb2d326ac5), deploy into slot with allocation id 7245d44b3a0d964ff0ad67ea1b4b9732.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 (b4c8639dab83f8b49aeea6eb2d326ac5) switched from CREATED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot e6c0f3ef4532332ad5fcd476d994899b.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 (b4c8639dab83f8b49aeea6eb2d326ac5) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 (d47d6c0a835426f2fb0e299cdd849b91), deploy into slot with allocation id e6c0f3ef4532332ad5fcd476d994899b.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 003fdcaadfd178da3b6bbe02f84a3e08.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 (d47d6c0a835426f2fb0e299cdd849b91) switched from CREATED to DEPLOYING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 (d47d6c0a835426f2fb0e299cdd849b91) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 (dd50a8b7ca589e10920eb3397cb2ac9a), deploy into slot with allocation id 003fdcaadfd178da3b6bbe02f84a3e08.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 (dd50a8b7ca589e10920eb3397cb2ac9a) switched from CREATED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot f201de2af4761175dbad50d2e4890168.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 (dd50a8b7ca589e10920eb3397cb2ac9a) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 (c3d9aeada70e62e32b083bfeb30f5b22), deploy into slot with allocation id f201de2af4761175dbad50d2e4890168.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 (c3d9aeada70e62e32b083bfeb30f5b22) switched from CREATED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 89ac6a3d19e1dd1198ab199aecf497cc.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 (c3d9aeada70e62e32b083bfeb30f5b22) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 (a315543da7d12f808d3f554fb8925d81), deploy into slot with allocation id 89ac6a3d19e1dd1198ab199aecf497cc.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 (a315543da7d12f808d3f554fb8925d81) switched from CREATED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 7693dd9bcd746b8f80444bd4dce4db49.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 (a315543da7d12f808d3f554fb8925d81) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 (441e95451059bc35c9657196f7fdd738), deploy into slot with allocation id 7693dd9bcd746b8f80444bd4dce4db49.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 (441e95451059bc35c9657196f7fdd738) switched from CREATED to DEPLOYING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot fd65528b142e9c9fc19b68b5576bcc24.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 (441e95451059bc35c9657196f7fdd738) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (95daf7b0d22323bd8cbb466609de7a45), deploy into slot with allocation id fd65528b142e9c9fc19b68b5576bcc24.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (95daf7b0d22323bd8cbb466609de7a45) switched from CREATED to DEPLOYING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@215fc475
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@319d58f0
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@77835233
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@17fd2072
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@1dcd8cad
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 0d5d849d9b1e25c938da99e7ddc5c140.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@375f517f
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (95daf7b0d22323bd8cbb466609de7a45) [DEPLOYING].
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@589fd2e3
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.TaskExecutor - Received task Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (3b6a34431045d6ae02f4acdf325be9b8), deploy into slot with allocation id 0d5d849d9b1e25c938da99e7ddc5c140.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (3b6a34431045d6ae02f4acdf325be9b8) switched from CREATED to DEPLOYING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Loading JAR files for task Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (3b6a34431045d6ae02f4acdf325be9b8) [DEPLOYING].
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 7245d44b3a0d964ff0ad67ea1b4b9732.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot e6c0f3ef4532332ad5fcd476d994899b.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 003fdcaadfd178da3b6bbe02f84a3e08.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot f201de2af4761175dbad50d2e4890168.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 89ac6a3d19e1dd1198ab199aecf497cc.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 7693dd9bcd746b8f80444bd4dce4db49.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot fd65528b142e9c9fc19b68b5576bcc24.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@54fec4f0
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.runtime.tasks.StreamTask - Checkpoint storage is set to 'jobmanager'
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl - Activate slot 0d5d849d9b1e25c938da99e7ddc5c140.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 (441e95451059bc35c9657196f7fdd738) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 (dd50a8b7ca589e10920eb3397cb2ac9a) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 (d47d6c0a835426f2fb0e299cdd849b91) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 (3b6a34431045d6ae02f4acdf325be9b8) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 (a315543da7d12f808d3f554fb8925d81) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 (c3d9aeada70e62e32b083bfeb30f5b22) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 (b4c8639dab83f8b49aeea6eb2d326ac5) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.runtime.taskmanager.Task - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 (95daf7b0d22323bd8cbb466609de7a45) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8) (441e95451059bc35c9657196f7fdd738) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8) (dd50a8b7ca589e10920eb3397cb2ac9a) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8) (d47d6c0a835426f2fb0e299cdd849b91) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8) (3b6a34431045d6ae02f4acdf325be9b8) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8) (a315543da7d12f808d3f554fb8925d81) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8) (c3d9aeada70e62e32b083bfeb30f5b22) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8) (b4c8639dab83f8b49aeea6eb2d326ac5) switched from DEPLOYING to INITIALIZING.
INFO flink-akka.actor.default-dispatcher-2 org.apache.flink.runtime.executiongraph.ExecutionGraph - Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8) (95daf7b0d22323bd8cbb466609de7a45) switched from DEPLOYING to INITIALIZING.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 0 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 1 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 4 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 2 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 7 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 6 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 3 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 5 has no restore state.
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (1/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (4/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (3/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (2/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (6/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (8/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (5/8)#0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [172.21.21.16:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = flink
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.11.0.2
INFO Source: Custom Source -> Map -> Sink: Print to Std. Out (7/8)#0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : 73be1e1168f91ee2
